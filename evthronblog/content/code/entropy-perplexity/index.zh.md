---
title: "熵和困惑度"
description: 
date: 2025-05-19T10:41:21+08:00
lastmod: 2025-05-19T10:41:21+08:00
image: 
categories: 
tags: 
math: true
license: 
hidden: false
comments: true
---

## 熵
雖然我的數學不差，但還是不得不承認，我只要看見對數就不爽。不僅意義很難懂，而且往往只是因為「這個公式是指數函數，用起來很不方便，所以我們就加個 log 把它壓平，變成線性函數」，就拿來用了，沒有什麼深層次的原因。

同樣的事情也發生在熵上。熵，Entropy，這個聽起來很帥的科幻術語，有着一條考試時背不出來的公式。

$$
H(P) = -\sum_{i=1}^n p(i) \log(p(i))
$$

概率乘以概率的對數？這也太方便了。又是一個公式很簡單所以沒理由答不上來，但真要解釋是什麼意思就一頭霧水的典型例子。

$$
H(P) = \sum_{i=1}^n p(i) \log(\frac{1}{p(i)})
$$

p 是事情發生的概率。那麼 1/p 是什麼意思呢？概率越高，稀有度越低，概率越低，稀有度越高，所以可以用 p 的倒數 1/p 來代表稀有度。

讓我們再來想想 1/p 是什麼意思。拋硬幣的時候，每面發生的機會是 1/2， 1 / 1/2 = 2，是一正一反，結果的數量。很容易理解吧？這個 2 就是困惑度，也就是事件量，意思是這個事件有兩個可能的結果。

兩個可能的結果，能用一個比特編碼。四個可能的結果，能用兩個比特編碼，所以編碼需要的位數，等於 $\log_2(\frac{1}{p})$，這就是 log 的來源。

熵的意思是編碼需要的位數，而困惑度的意思是總共有多少個結果，所以就用 $2^{H(P)}$ 計算。

熵的定義，是依賴於每個位能表達多少個訊息的。如果一個位可以表達兩種狀態，就用 $\log_2$，如果一個位可以表達三種狀態，就用 $\log_3$。如果一個位可以表達$e$種狀態，就用 $\ln$。

但是，困惑度是不會改變的，無論用多少個位表達，結果的數量是不會變的，似乎應該是更加基礎的概念，但我們之所以用熵而不是用困惑度表示資訊量，是因為結果的數量有着麻煩的指數性質，而編碼需要的位數則能漂亮地相加，更容易理解。

## 困惑度

我試着無視課本，脱離熵的定義，直接求困惑度。

接下來要舉的例子可能有點多餘，但對理解接下來的不平均概率分佈有幫助。

讓我們拋兩次硬幣，拋一次會有兩種結果，所以拋兩次會有 2 x 2 = 4 種結果。那麼，平均拋一次的結果數是多少呢？是 4 的開平方，也就是回到了一開始的 2。如果拋三次硬幣就會有 2 x 2 x 2 = 8 種結果，平均拋一次的結果數是 8 的 開立方，一樣是 2。

接下來是不平均的概率分佈，我們現在有一個不公平的硬幣，有 1/4 的可能是正面，有 3/4 的可能是反面。現在我們有多少個事件呢？雖然只有兩個結果，但是這個概率分佈的隨機性是比公平的硬幣低的，比起正面，更有可能是反面，所以實際預期的平均事件數，也就是不確定性，應該是比 2 少的。

用剛才的做法，現在我們拋四次這個硬幣，有多少種可能的結果呢？先算正面，事件數是 $1/\frac{1}{4} = 4$，也就是説，每 4 次就會出現一次正面，正面的結果會發生多少次呢？四次裏面會有一次，所以自乘一次，還是 $4$。

反面呢？這次的事件數是 $1/\frac{3}{4} = \frac{4}{3}$，不是整數！意思是，如果一個事件發生的概率是 $\frac{3}{4}$，那我們預期每 $\frac{4}{3}$ 次就會發生一次事件，反面的結果會發生 3 次，所以要計算 $(\frac{4}{3})^3 \approx 2.37$，意思是，如果我把拋 3 次硬幣當作一次實驗，我們可以預期每重複 2.37 次實驗就會出現一次三個反面。

這樣一來，我們就得到了一次正面和三次反面，我們把這個事件叫作「典型事件」。為了得到這個「典型事件」，我們需要等待 $4 \times 2.37 = 9.481$，意思是，一次拋四個硬幣，每 9.481 次就會得到一次「典型事件」。（雖然典型事件有四種排列組合，但只用一個就能算得出來。我不是很清楚為什麼。）

雖然「典型事件」至少需要拋四次才能發生，但抽象地説，如果只拋一次，要多少次才能得到一個「典型事件」呢？答案會是 9.481 的開四次方，也就是 1.754。這就是預期的平均事件數，如果發生的可能事件越多，不確定性就越大，這個事件數就是「困惑度」。拋這個不公平硬幣的不確定性，相當於拋一個只有 1.754 個面的公平硬幣。

按照上面這個可能有漏洞的推論，我們得到了不依賴熵來定義困惑度的公式：

$$
\text{Perplexity} = \sqrt[N]{\prod_{i=1}^n \left(\frac{1}{p_i}\right)^{k_i}}
$$

其中：
- $p_i$ 是事件 $i$ 的概率
- $k_i$ 是事件 $i$ 發生的次數
- $N$ 是總實驗次數

或者用更簡單的寫法：
$$
\text{Perplexity} = \prod_{i=1}^n \left(\frac{1}{p_i}\right)^{p_i}
$$

$$
H(P) = \sum_{i=1}^n p(i) \log(\frac{1}{p(i)})
$$

這和熵的公式只差了一個 log，可以看見真正的資訊量有相乘的性質，很麻煩。這甚至是我第一次看見幾何平均，最大的問題是概率相乘會變得很小，電腦的精度不夠。而編碼資訊需要的比特數則有着相加的性質，更加好用。所以用熵來定義困惑度感覺是一種運算上的技巧。

## 交叉熵
交叉熵能對比兩個概率分佈的相似度。
$$
CE(P, Q) = \sum_{i=1}^n \text{actualProbability}(i) \log(\frac{1}{\text{predictionProbability}(i)})
$$

交叉熵同樣是熵，所以意義也是編碼需要的比特量。一般的熵代表的是編碼需要的最小比特數，而交叉熵就説明了當我們的概率預測出錯的時候，實際需要的比特數。要讓編碼的比特數變得最少，就要給發生概率高的事件分配短一點的編碼，給發生概率低的事件用長一點的編碼也沒關係，如果對於一件發生概率很高的事件，我們卻以為發生概率很低，$\log(\frac{1}{p(i)})$ 就會變大，會分配多了好幾個比特用來編碼，訊息就會變得很冗長。因此降低交叉熵就能讓預測的概率接近正確的概率。








